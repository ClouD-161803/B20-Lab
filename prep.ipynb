{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Check if PyTorch is using GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create a sample tensor with PyTorch\n",
    "tensor = torch.tensor([1, 2, 3], device=device)\n",
    "print(f\"Sample tensor: {tensor}\")\n",
    "\n",
    "# Create a sample array with NumPy\n",
    "array = np.array([1, 2, 3])\n",
    "print(f\"Sample array: {array}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "## Linear Regression with Ridge and Lasso\n",
    "\n",
    "In this section, we will set up the infrastructure to perform linear regression using both Ridge and Lasso methods. We will use PyTorch for this purpose.\n",
    "\n",
    "1. **Import necessary libraries**: We will use PyTorch for tensor operations and model building, and NumPy for array manipulations.\n",
    "2. **Generate synthetic data**: Create a dataset for regression.\n",
    "3. **Define the model**: Create linear regression models with Ridge and Lasso regularization.\n",
    "4. **Train the model**: Implement the training loop.\n",
    "5. **Evaluate the model**: Assess the performance of the models.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(100, 1)\n",
    "y = 3 * X.squeeze() + 2 + np.random.randn(100)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the models\n",
    "ridge = Ridge(alpha=1.0)\n",
    "lasso = Lasso(alpha=0.1)\n",
    "\n",
    "# Train the models\n",
    "ridge.fit(X_train, y_train)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "\n",
    "print(f\"Ridge MSE: {mse_ridge}\")\n",
    "print(f\"Lasso MSE: {mse_lasso}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the models with different kernels\n",
    "svm_linear = SVC(kernel='linear')\n",
    "svm_poly = SVC(kernel='poly', degree=3)\n",
    "svm_rbf = SVC(kernel='rbf')\n",
    "\n",
    "# Train the models\n",
    "svm_linear.fit(X_train, y_train)\n",
    "svm_poly.fit(X_train, y_train)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_linear = svm_linear.predict(X_test)\n",
    "y_pred_poly = svm_poly.predict(X_test)\n",
    "y_pred_rbf = svm_rbf.predict(X_test)\n",
    "\n",
    "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
    "accuracy_poly = accuracy_score(y_test, y_pred_poly)\n",
    "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "\n",
    "print(f\"Linear SVM Accuracy: {accuracy_linear}\")\n",
    "print(f\"Polynomial SVM Accuracy: {accuracy_poly}\")\n",
    "print(f\"Gaussian SVM Accuracy: {accuracy_rbf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(100, 1)\n",
    "\n",
    "# Define a simple probabilistic model\n",
    "class ProbabilisticModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProbabilisticModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = ProbabilisticModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(torch.tensor(X, dtype=torch.float32))\n",
    "    loss = criterion(outputs, torch.tensor(X, dtype=torch.float32))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Implement Laplace approximation\n",
    "def laplace_approximation(model, data):\n",
    "    model.eval()\n",
    "    outputs = model(torch.tensor(data, dtype=torch.float32))\n",
    "    loss = criterion(outputs, torch.tensor(data, dtype=torch.float32))\n",
    "    loss.backward()\n",
    "    \n",
    "    # Compute the Hessian\n",
    "    hessian = []\n",
    "    for param in model.parameters():\n",
    "        hessian.append(param.grad.data.view(-1))\n",
    "    hessian = torch.cat(hessian)\n",
    "    \n",
    "    # Approximate the posterior\n",
    "    posterior_mean = torch.cat([param.data.view(-1) for param in model.parameters()])\n",
    "    posterior_cov = torch.inverse(hessian)\n",
    "    \n",
    "    return posterior_mean, posterior_cov\n",
    "\n",
    "# Evaluate the model\n",
    "posterior_mean, posterior_cov = laplace_approximation(model, X)\n",
    "print(f\"Posterior Mean: {posterior_mean}\")\n",
    "print(f\"Posterior Covariance: {posterior_cov}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Inverse Transform Sampling\n",
    "def inverse_transform_sampling(cdf, size=1):\n",
    "    uniform_samples = np.random.uniform(0, 1, size)\n",
    "    return np.interp(uniform_samples, cdf, np.linspace(0, 1, len(cdf)))\n",
    "\n",
    "# Rejection Sampling\n",
    "def rejection_sampling(pdf, size=1, M=1):\n",
    "    samples = []\n",
    "    while len(samples) < size:\n",
    "        x = np.random.uniform(0, 1)\n",
    "        y = np.random.uniform(0, M)\n",
    "        if y < pdf(x):\n",
    "            samples.append(x)\n",
    "    return np.array(samples)\n",
    "\n",
    "# Metropolis Sampling\n",
    "def metropolis_sampling(pdf, size=1, start=0.5, proposal_width=0.1):\n",
    "    samples = [start]\n",
    "    current = start\n",
    "    for _ in range(size - 1):\n",
    "        proposal = np.random.normal(current, proposal_width)\n",
    "        acceptance_ratio = min(1, pdf(proposal) / pdf(current))\n",
    "        if np.random.uniform(0, 1) < acceptance_ratio:\n",
    "            current = proposal\n",
    "        samples.append(current)\n",
    "    return np.array(samples)\n",
    "\n",
    "# Example usage\n",
    "def example_pdf(x):\n",
    "    return np.exp(-x**2 / 2) / np.sqrt(2 * np.pi)\n",
    "\n",
    "def example_cdf(x):\n",
    "    return (1 + np.erf(x / np.sqrt(2))) / 2\n",
    "\n",
    "# Generate samples\n",
    "inverse_samples = inverse_transform_sampling(example_cdf(np.linspace(-3, 3, 1000)), size=1000)\n",
    "rejection_samples = rejection_sampling(example_pdf, size=1000, M=1)\n",
    "metropolis_samples = metropolis_sampling(example_pdf, size=1000)\n",
    "\n",
    "# Plot the samples\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(inverse_samples, bins=30, density=True)\n",
    "plt.title('Inverse Transform Sampling')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(rejection_samples, bins=30, density=True)\n",
    "plt.title('Rejection Sampling')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(metropolis_samples, bins=30, density=True)\n",
    "plt.title('Metropolis Sampling')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Define the loss function\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Example usage\n",
    "input_dim = 784  # for MNIST dataset\n",
    "hidden_dim = 400\n",
    "latent_dim = 20\n",
    "vae = VAE(input_dim, hidden_dim, latent_dim)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.view(-1, input_dim)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae(data)\n",
    "        loss = vae_loss(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch}, Loss: {train_loss / len(train_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a2e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
